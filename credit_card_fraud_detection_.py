# -*- coding: utf-8 -*-
"""Credit Card Fraud Detection .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bjTlzW8k-CWbqqjcZTgalrZU6e7uF9Py

# **Credit Card Fraud Detection using Scikit-Learn**

we will use a real dataset to train each of these models. The dataset includes information about transactions made by credit cards in September 2013 by European cardholders. You will use the trained model to assess if a credit card transaction is legitimate or not.

<p>Import libraries<p>
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import normalize, StandardScaler
from sklearn.utils.class_weight import compute_sample_weight
from sklearn.metrics import roc_auc_score
import time
import warnings
import seaborn as sns
import matplotlib.pyplot as plt

"""Import the data from github link"""

import pandas as pd
link='https://raw.githubusercontent.com/nsethi31/Kaggle-Data-Credit-Card-Fraud-Detection/master/creditcard.csv'
df=pd.read_csv(link)
df.head(15)

df.info()

"""Divide th Data Input and Target"""

X=df.drop(['Class'], axis=1)
y=df['Class']
X

sns.set()
cols=X.columns
cols

"""Check for outliers in the V1 column"""

plt.plot(df['Time'],df['V1'])

"""we found that there are some out liers which are lesser than -50. so i wanna remove them"""

out=df[df['V1']<-50]
out

df2=df.copy()
df2.head()

"""Removing the outlier"""

df2=df2.drop([39769])
df2.shape

"""I cannot go through all the columns and hardcode it. So will loop them in for loop to see the outliers"""

cols=cols[2:-1]
cols

for col in cols:
  plt.plot(df['Time'],df[col])

"""we found some values are too higher than other so will remove them by running a for loop"""

for col in cols:
    df2 = df2[~((df2[col] < -50) | (df2[col] > 50))]

df2.shape

"""Check again how the data looks like after removing outliers.Now its looking good."""

for col in cols:
  plt.plot(df2['Time'],df2[col])

"""check for Data Imbalance...!
It seems like there is huge Data Imbalance in our data set.
Now we need to handel it
"""

df2.Class.value_counts()

"""Divide our data into two separate classes based on Class"""

majorty_samples,minority_samples=df2.Class.value_counts()
majorty_samples,minority_samples

majorty_class=df2[df2['Class']==0]
minority_class=df2[df2['Class']==1]

"""Use SMOTE to over sample the minority class Data"""

from imblearn.over_sampling import SMOTE
smote=SMOTE(sampling_strategy='minority')
xsmt,ysmt=smote.fit_resample((df2.drop(['Class'],axis='columns')),df2.Class)

xsmt.shape,ysmt.shape

"""now the data is balanced"""

ysmt.value_counts()

from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report,accuracy_score,confusion_matrix

"""Define funcion to create modeland generating accuracy_score, Classification report and confusion matrix"""

def model(X,y):
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
  rfc=RandomForestClassifier()
  rfc.fit(X_train,y_train)
  ypred=rfc.predict(X_test)
  return accuracy_score(y_test,ypred),classification_report(y_test,ypred),confusion_matrix(y_test,ypred)

"""I wanna check our model performance on both Imbanced Data and Balanced.
First check the model performance on Imbanced Data
"""

ac,cr,cm=model(df.drop(['Class'],axis='columns'),df.Class)

"""Accuracy is pretty good, but in the feild of finance there will be only very few fraud transactions will happen like 1 in thousands.If our model not able detect that one transaction there is use."""

ac

"""precision and accuracy are good and we need to improve recall and f1_score"""

print(cr)

"""out model 22 fake transactions as fair its good i think"""

ax = sns.heatmap(cm, annot=True, fmt='d', )
plt.show(ax)

"""<h1>Now run the model on Balanced Data<h1>"""

ac_processed,cr_processed,cm_processed=model(xsmt,ysmt)

"""there is an improvement in accuracy"""

ac_processed

"""upon comparing both models, model generated very high precission results on Balanced data than Imbalanced data"""

print(cr_processed)

cx = sns.heatmap(cm_processed, annot=True, fmt='d', )
plt.show(cx)

